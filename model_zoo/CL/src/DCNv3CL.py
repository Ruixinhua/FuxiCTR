# =========================================================================
# Copyright (C) 2024. The FuxiCTR Library. All rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========================================================================

"""
DCNv3CL: DCNv3 with Contrastive Learning

åŸºäºDCNv3æ¨¡å‹å®ç°çš„å¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬ï¼Œæ”¯æŒï¼š
1. ä¸ªæ€§åŒ–ç‰¹å¾æ©ç 
2. ç‰¹å¾å¯¹é½æŸå¤±  
3. å­—æ®µå‡åŒ€æ€§æŸå¤±
4. è·ç¦»æŸå¤±
5. å¤šå¡”ç»“æ„ (MT)
"""

import torch
import logging


from ...DCNv3.src import DCNv3
from .base import ContrastiveLearningBase


class DCNv3CL(DCNv3, ContrastiveLearningBase):
    """
    DCNv3 with Contrastive Learning
    
    ç»“åˆDCNv3çš„æ·±åº¦äº¤å‰ç½‘ç»œæ¶æ„å’Œå¯¹æ¯”å­¦ä¹ çš„å¢å¼ºåŠŸèƒ½
    æ”¯æŒå¤šå¡”ç»“æ„å’Œå¯¹æ¯”å­¦ä¹ çš„è”åˆè®­ç»ƒ
    """
    
    def __init__(self,
                 feature_map,
                 model_id="DCNv3CL",
                 gpu=-1,
                 learning_rate=1e-3,
                 embedding_dim=10,
                 num_deep_cross_layers=4,
                 num_shallow_cross_layers=4,
                 deep_net_dropout=0.1,
                 shallow_net_dropout=0.3,
                 layer_norm=True,
                 batch_norm=False,
                 num_heads=1,
                 embedding_regularizer=None,
                 net_regularizer=None,
                 use_domain_aware_structure=False,
                 # CLç›¸å…³å‚æ•°
                 cl_config=None,
                 **kwargs):
        
        # åˆå§‹åŒ–ContrastiveLearningBase
        ContrastiveLearningBase.__init__(self, cl_config=cl_config, **kwargs)
        
        # åˆå§‹åŒ–DCNv3
        DCNv3.__init__(self,
                      feature_map=feature_map,
                      model_id=model_id,
                      gpu=gpu,
                      learning_rate=learning_rate,
                      embedding_dim=embedding_dim,
                      num_deep_cross_layers=num_deep_cross_layers,
                      num_shallow_cross_layers=num_shallow_cross_layers,
                      deep_net_dropout=deep_net_dropout,
                      shallow_net_dropout=shallow_net_dropout,
                      layer_norm=layer_norm,
                      batch_norm=batch_norm,
                      num_heads=num_heads,
                      embedding_regularizer=embedding_regularizer,
                      net_regularizer=net_regularizer,
                      use_domain_aware_structure=use_domain_aware_structure,
                      **kwargs)
        
        logging.info(f"DCNv3CLæ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚CLé…ç½®: {self.cl_config}")
        logging.info(f"ä½¿ç”¨åŸŸæ„ŸçŸ¥ç»“æ„(MT): {self.use_domain_aware_structure}")
    
    def _get_dcnv3_logits(self, X):
        """
        è·å–DCNv3æ¨¡å‹çš„logitsï¼ˆä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼‰
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            torch.Tensor: åŸå§‹logits
        """
        feature_emb = self.embedding_layer(X)

        if self.use_domain_aware_structure:
            xld_intermediate = self.ECN(feature_emb)
            xls_intermediate = self.LCN(feature_emb)

            xld_flat = xld_intermediate.view(xld_intermediate.size(0), -1)
            xls_flat = xls_intermediate.view(xls_intermediate.size(0), -1)

            logits_xld = self._generate_domain_aware_logits_pytorch(X, xld_flat)
            logits_xls = self._generate_domain_aware_logits_pytorch(X, xls_flat)
        else:
            logits_xld = self.ECN(feature_emb).mean(dim=1)
            logits_xls = self.LCN(feature_emb).mean(dim=1)
        
        # åˆå¹¶ECNå’ŒLCNçš„è¾“å‡ºä½œä¸ºæœ€ç»ˆlogits
        logits = (logits_xld + logits_xls) * 0.5
        
        return logits
    
    def _get_base_model_logits(self, X, base_model=None, **kwargs):
        """
        å®ç°ContrastiveLearningBaseè¦æ±‚çš„æŠ½è±¡æ–¹æ³•
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            base_model: åŸºç¡€æ¨¡å‹ï¼ˆåœ¨è¿™é‡Œå¿½ç•¥ï¼Œç›´æ¥ä½¿ç”¨selfï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            torch.Tensor: logits
        """
        return self._get_dcnv3_logits(X)
    
    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­ï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ åŠŸèƒ½
        
        Args:
            inputs: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„å­—å…¸
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œä¸­é—´ç»“æœçš„å­—å…¸
        """
        X = self.get_inputs(inputs)
        
        # è·å–DCNv3çš„å®Œæ•´å‰å‘ä¼ æ’­ç»“æœ
        dcnv3_output = super().forward(inputs)
        
        # ä»çˆ¶ç±»çš„å®ä¾‹å˜é‡ä¸­è·å–logitsï¼Œé¿å…é‡å¤è®¡ç®—
        # çˆ¶ç±»DCNv3åœ¨forwardä¸­å·²ç»è®¡ç®—äº†self.logits_xldå’Œself.logits_xls
        base_logits = (self.logits_xld + self.logits_xls) * 0.5
        
        # æ·»åŠ logitsåˆ°è¿”å›å­—å…¸
        dcnv3_output["logits"] = base_logits
        
        # è·å–ç»„ä¿¡æ¯ï¼ˆåŸºäºis_personalizationç‰¹å¾ï¼‰
        if self.training:
            group_ids = self.get_group_ids(inputs)
            if group_ids is not None:
                dcnv3_output["group_ids"] = group_ids
        
        # å¦‚æœå¯ç”¨CLï¼Œä»å·²è®¡ç®—çš„embeddingä¸­æå–ç‰¹å¾åµŒå…¥ï¼Œé¿å…é‡å¤è®¡ç®—
        if self.training and (self.feature_alignment_loss_weight > 0 or 
                             self.field_uniformity_loss_weight > 0):
            # å¤ç”¨å·²ç»è®¡ç®—çš„embeddingï¼Œé¿å…é‡æ–°è®¡ç®—
            feature_emb = self.embedding_layer(X)
            feature_embeddings = self._extract_feature_embeddings_from_tensor(feature_emb, X)
            dcnv3_output["feature_embeddings"] = feature_embeddings
        
        # å¦‚æœå¯ç”¨ä¸ªæ€§åŒ–æ©ç ï¼Œç”Ÿæˆå¯¹æ¯”è§†å›¾ï¼Œå¤ç”¨å·²è®¡ç®—çš„logits
        if self.training and self.use_cl_mask:
            # h1ä½¿ç”¨å·²è®¡ç®—çš„logitsï¼Œé¿å…é‡å¤è®¡ç®—
            h1_logits = base_logits
            
            # h2åªéœ€è¦è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
            h2_logits = self._compute_non_personalized_logits(X)
            
            if h1_logits is not None and h2_logits is not None:
                dcnv3_output["h1_logits"] = h1_logits
                dcnv3_output["h2_logits"] = h2_logits
        
        return dcnv3_output
    
    def _extract_feature_embeddings_from_tensor(self, feature_emb, X):
        """
        ä»å·²è®¡ç®—çš„ç‰¹å¾åµŒå…¥å¼ é‡ä¸­æå–å„ä¸ªç‰¹å¾çš„åµŒå…¥ï¼Œé¿å…é‡å¤è®¡ç®—
        
        Args:
            feature_emb: å·²è®¡ç®—çš„ç‰¹å¾åµŒå…¥å¼ é‡ [batch_size, num_fields, embedding_dim]
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            dict: {feature_name: embedding_tensor}
        """
        feature_embeddings = {}
        feature_names = list(X.keys())
        
        # å°†ç‰¹å¾åµŒå…¥æŒ‰å­—æ®µåˆ†å‰²
        if feature_emb.dim() == 3:  # [batch_size, num_fields, embedding_dim]
            feat_list = feature_emb.chunk(len(feature_names), dim=1)
            for i, feature_name in enumerate(feature_names):
                if i < len(feat_list):
                    feature_embeddings[feature_name] = feat_list[i].squeeze(1)
        else:
            # å¦‚æœæ˜¯å±•å¹³çš„ï¼Œéœ€è¦é‡æ–°è®¡ç®—å„ä¸ªç‰¹å¾çš„åµŒå…¥
            feature_embeddings = self.get_feature_embeddings(self.embedding_layer, X)
            
        return feature_embeddings
    
    def _compute_non_personalized_logits(self, X):
        """
        è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            torch.Tensor: éä¸ªæ€§åŒ–è§†å›¾çš„logits
        """
        if not self.use_cl_mask or self.mask_type != 'Personalisation':
            return None
        
        # ç”Ÿæˆéä¸ªæ€§åŒ–è§†å›¾
        non_personalized_X = {}
        for feature_name, feature_value in X.items():
            if feature_name in self.personalization_feature_list:
                # ä½¿ç”¨ç½®é›¶ç­–ç•¥ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•å…¶ä»–ç­–ç•¥ï¼‰
                non_personalized_X[feature_name] = torch.zeros_like(feature_value)
            else:
                non_personalized_X[feature_name] = feature_value
        
        # è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
        return self._get_dcnv3_logits(non_personalized_X)
    
    def add_loss(self, return_dict, y_true):
        """
        è®¡ç®—åŒ…å«å¯¹æ¯”å­¦ä¹ çš„æ€»æŸå¤±
        
        Args:
            return_dict: forwardæ–¹æ³•çš„è¿”å›å­—å…¸
            y_true: çœŸå®æ ‡ç­¾
            
        Returns:
            torch.Tensor: æ€»æŸå¤±
        """
        # è·å–DCNv3çš„åŸºç¡€æŸå¤±ï¼ˆåŒ…å«æ·±æµ…å±‚æŸå¤±ï¼‰
        dcnv3_base_loss = super().add_loss(return_dict, y_true)
        
        # å¦‚æœä¸åœ¨è®­ç»ƒæ¨¡å¼æˆ–æ²¡æœ‰å¯ç”¨ä»»ä½•CLæŸå¤±ï¼Œç›´æ¥è¿”å›DCNv3åŸºç¡€æŸå¤±
        if not self.training or not self.use_cl_loss:
            return dcnv3_base_loss
        
        # æå–CLç›¸å…³çš„ä¸­é—´ç»“æœ
        feature_embeddings = return_dict.get("feature_embeddings", None)
        h1_logits = return_dict.get("h1_logits", None)
        h2_logits = return_dict.get("h2_logits", None)
        group_ids = return_dict.get("group_ids", None)
        
        # ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„compute_cl_lossæ–¹æ³•è®¡ç®—å®Œæ•´çš„CLæŸå¤±
        total_loss = self.compute_cl_loss(
            base_loss=dcnv3_base_loss,
            feature_embeddings=feature_embeddings,
            h1_logits=h1_logits,
            h2_logits=h2_logits,
            labels=y_true,
            group_ids=group_ids
        )
        
        return total_loss 