# =========================================================================
# Copyright (C) 2024. The FuxiCTR Library. All rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========================================================================

"""
DCNv2CL: DCNv2 with Contrastive Learning

åŸºäºDCNv2WithMultiToweræ¨¡å‹å®ç°çš„å¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬ï¼Œæ”¯æŒï¼š
1. ä¸ªæ€§åŒ–ç‰¹å¾æ©ç 
2. ç‰¹å¾å¯¹é½æŸå¤±  
3. å­—æ®µå‡åŒ€æ€§æŸå¤±
4. è·ç¦»æŸå¤±
5. å¤šç§ç½‘ç»œç»“æ„ (parallel, stacked, crossnet_only)
6. ğŸ¯ å¤šå¡”(MT)æ”¯æŒï¼ˆé€šè¿‡ç»§æ‰¿DCNv2WithMultiTowerå®ç°ï¼‰
"""

import torch
import logging

# å¯¼å…¥DCNv2WithMultiToweræ¨¡å‹
from ...DCNv2.src.DCNv2MT import DCNv2WithMultiTower
from .base import ContrastiveLearningBase


class DCNv2CL(DCNv2WithMultiTower, ContrastiveLearningBase):
    """
    DCNv2 with Contrastive Learning
    
    ç»§æ‰¿è‡ªDCNv2WithMultiTowerï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ åŠŸèƒ½
    æ”¯æŒå¤šç§ç½‘ç»œç»“æ„æ¨¡å¼ã€å¯¹æ¯”å­¦ä¹ å’ŒåŸŸæ„ŸçŸ¥ç»“æ„çš„è”åˆè®­ç»ƒ
    ğŸ¯ é€šè¿‡çˆ¶ç±»æ”¯æŒå¤šå¡”(MT)ç»“æ„
    """
    
    def __init__(self, 
                 feature_map, 
                 model_id="DCNv2CL", 
                 gpu=-1,
                 model_structure="parallel",
                 use_low_rank_mixture=False,
                 low_rank=32,
                 num_experts=4,
                 learning_rate=1e-3, 
                 embedding_dim=10, 
                 stacked_dnn_hidden_units=[], 
                 parallel_dnn_hidden_units=[],
                 dnn_activations="ReLU",
                 num_cross_layers=3,
                 net_dropout=0, 
                 batch_norm=False, 
                 embedding_regularizer=None,
                 net_regularizer=None,
                 # CLç›¸å…³å‚æ•°
                 cl_config=None,
                 # MTç›¸å…³å‚æ•°ï¼ˆç»§æ‰¿è‡ªDCNv2WithMultiTowerï¼‰
                 use_domain_aware_structure=False,
                 tower_hidden_units_list=None,
                 tower_activation='ReLU',
                 tower_l2_reg_list=None,
                 tower_dropout_list=None,
                 use_bn_tower=True,
                 scene_name='scene_id',
                 scene_num_shift=1,
                 use_scene_id_mapping=False,
                 mapping_feature_name=None,
                 mapping_feature_type=None,
                 feature2id_dict=None,
                 default_value=None,
                 feature_map_dict=None,
                 **kwargs):
        
        # åˆå§‹åŒ–ContrastiveLearningBase
        ContrastiveLearningBase.__init__(self, cl_config=cl_config, **kwargs)
        
        # åˆå§‹åŒ–DCNv2WithMultiTowerï¼ˆåŒ…å«æ‰€æœ‰MTç›¸å…³åŠŸèƒ½ï¼‰
        DCNv2WithMultiTower.__init__(self,
                                   feature_map=feature_map,
                                   model_id=model_id,
                                   gpu=gpu,
                                   learning_rate=learning_rate,
                                   model_structure=model_structure,
                                   use_low_rank_mixture=use_low_rank_mixture,
                                   low_rank=low_rank,
                                   num_experts=num_experts,
                                   embedding_dim=embedding_dim,
                                   stacked_dnn_hidden_units=stacked_dnn_hidden_units,
                                   parallel_dnn_hidden_units=parallel_dnn_hidden_units,
                                   dnn_activations=dnn_activations,
                                   num_cross_layers=num_cross_layers,
                                   net_dropout=net_dropout,
                                   batch_norm=batch_norm,
                                   embedding_regularizer=embedding_regularizer,
                                   net_regularizer=net_regularizer,
                                   # MTç›¸å…³å‚æ•°
                                   use_domain_aware_structure=use_domain_aware_structure,
                                   tower_hidden_units_list=tower_hidden_units_list,
                                   tower_activation=tower_activation,
                                   tower_l2_reg_list=tower_l2_reg_list,
                                   tower_dropout_list=tower_dropout_list,
                                   use_bn_tower=use_bn_tower,
                                   scene_name=scene_name,
                                   scene_num_shift=scene_num_shift,
                                   use_scene_id_mapping=use_scene_id_mapping,
                                   mapping_feature_name=mapping_feature_name,
                                   mapping_feature_type=mapping_feature_type,
                                   feature2id_dict=feature2id_dict,
                                   default_value=default_value,
                                   feature_map_dict=feature_map_dict,
                                   **kwargs)
        
        logging.info(f"DCNv2CLæ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚CLé…ç½®: {self.cl_config}")
        logging.info(f"ä½¿ç”¨ç½‘ç»œç»“æ„: {model_structure}")
        logging.info(f"ä½¿ç”¨åŸŸæ„ŸçŸ¥ç»“æ„(MT): {self.use_domain_aware_structure}")
    
    def _get_dcnv2_logits(self, X):
        """
        è·å–DCNv2æ¨¡å‹çš„logitsï¼ˆä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼‰
        é€‚é…DCNv2WithMultiTowerçš„ç»“æ„
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            torch.Tensor: åŸå§‹logits
        """
        if not self.use_domain_aware_structure:
            # éMTæ¨¡å¼ï¼šè·å–åŸå§‹DCNv2çš„logits
            feature_emb = self.embedding_layer(X, flatten_emb=True)
            cross_out = self.crossnet(feature_emb)
            
            if self.model_structure == "crossnet_only":
                final_out = cross_out
            elif self.model_structure == "stacked":
                final_out = self.stacked_dnn(cross_out)
            elif self.model_structure == "parallel":
                dnn_out = self.parallel_dnn(feature_emb)
                final_out = torch.cat([cross_out, dnn_out], dim=-1)
            elif self.model_structure == "stacked_parallel":
                final_out = torch.cat([self.stacked_dnn(cross_out), self.parallel_dnn(feature_emb)], dim=-1)
            else:
                raise ValueError(f"Unsupported model structure: {self.model_structure}")
            
            # è·å–æœ€ç»ˆå±‚çš„logitsï¼ˆä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼‰
            logits = self.fc(final_out)
            
            return logits
        else:
            # MTæ¨¡å¼ï¼šåˆ©ç”¨çˆ¶ç±»çš„å¤šå¡”ç»“æ„è·å–logits
            feature_emb = self.embedding_layer(X, flatten_emb=True)
            
            # é€šè¿‡DCNv2å¤„ç†ï¼Œè·å–ç‰¹å¾è¡¨ç¤ºï¼ˆä½¿ç”¨çˆ¶ç±»æ–¹æ³•ï¼‰
            dcnv2_features = self._get_dcnv2_features(feature_emb)
            
            # é€šè¿‡å¤šå¡”æ¨¡å—è·å–åŸå§‹logitsï¼ˆä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼‰
            logits = self.multi_tower_module(dcnv2_features, X)
            
            return logits
    
    def _get_base_model_logits(self, X, base_model=None, **kwargs):
        """
        å®ç°ContrastiveLearningBaseè¦æ±‚çš„æŠ½è±¡æ–¹æ³•
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            base_model: åŸºç¡€æ¨¡å‹ï¼ˆåœ¨è¿™é‡Œå¿½ç•¥ï¼Œç›´æ¥ä½¿ç”¨selfï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            torch.Tensor: logits
        """
        return self._get_dcnv2_logits(X)
    
    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­ï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ åŠŸèƒ½ï¼Œæ”¯æŒMT
        åˆ©ç”¨çˆ¶ç±»DCNv2WithMultiTowerçš„forwardé€»è¾‘
        
        Args:
            inputs: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„å­—å…¸
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œä¸­é—´ç»“æœçš„å­—å…¸
        """
        X = self.get_inputs(inputs)
        
        if not self.use_domain_aware_structure:
            # éMTæ¨¡å¼ï¼šä½¿ç”¨çˆ¶ç±»çš„åŸºç¡€forwardé€»è¾‘
            parent_result = super(DCNv2WithMultiTower, self).forward(inputs)
        else:
            # MTæ¨¡å¼ï¼šä½¿ç”¨çˆ¶ç±»çš„å®Œæ•´forwardé€»è¾‘
            parent_result = super().forward(inputs)
        
        return_dict = {"y_pred": parent_result["y_pred"], "logits": self._get_dcnv2_logits(X)}
        
        # ğŸ¯ å¯¹æ¯”å­¦ä¹ ç›¸å…³å¤„ç†ï¼ˆæ— è®ºæ˜¯å¦ä½¿ç”¨å¤šå¡”éƒ½æ”¯æŒï¼‰
        # è·å–ç»„ä¿¡æ¯ï¼ˆåŸºäºis_personalizationç‰¹å¾ï¼‰
        if self.training:
            group_ids = self.get_group_ids(inputs)
            if group_ids is not None:
                return_dict["group_ids"] = group_ids
        
        # å¦‚æœå¯ç”¨CLï¼Œè®¡ç®—é¢å¤–çš„CLç»„ä»¶
        if self.training and (self.feature_alignment_loss_weight > 0 or 
                             self.field_uniformity_loss_weight > 0):
            # è·å–å„ä¸ªç‰¹å¾çš„åµŒå…¥ç”¨äºè®¡ç®—CLæŸå¤±
            feature_embeddings = self.get_feature_embeddings(self.embedding_layer, X)
            return_dict["feature_embeddings"] = feature_embeddings
        
        # å¦‚æœå¯ç”¨ä¸ªæ€§åŒ–æ©ç ï¼Œç”Ÿæˆå¯¹æ¯”è§†å›¾
        # if self.training and self.use_cl_mask:
        #     h1_logits, h2_logits = self.apply_personalization_mask(X, base_model=self, **{})
        #     if h1_logits is not None and h2_logits is not None:
        #         return_dict["h1_logits"] = h1_logits
        #         return_dict["h2_logits"] = h2_logits
        
        return return_dict
    
    def add_loss(self, return_dict, y_true):
        """
        è®¡ç®—åŒ…å«å¯¹æ¯”å­¦ä¹ çš„æ€»æŸå¤±
        
        Args:
            return_dict: forwardæ–¹æ³•çš„è¿”å›å­—å…¸
            y_true: çœŸå®æ ‡ç­¾
            
        Returns:
            torch.Tensor: æ€»æŸå¤±
        """
        # åŸºç¡€DCNv2æŸå¤±
        base_loss = self.loss_fn(return_dict["y_pred"], y_true, reduction='mean')
        
        # å¦‚æœä¸åœ¨è®­ç»ƒæ¨¡å¼æˆ–æ²¡æœ‰å¯ç”¨ä»»ä½•CLæŸå¤±ï¼Œç›´æ¥è¿”å›åŸºç¡€æŸå¤±
        if not self.training or not self.use_cl_loss:
            return base_loss
        
        # æå–CLç›¸å…³çš„ä¸­é—´ç»“æœ
        feature_embeddings = return_dict.get("feature_embeddings", None)
        h1_logits = return_dict.get("h1_logits", None)
        h2_logits = return_dict.get("h2_logits", None)
        
        # ğŸš€ è·å–ç»„ä¿¡æ¯ï¼ˆåŸºäºis_personalizationç‰¹å¾ï¼‰
        # is_personalization=1: ä¸ªæ€§åŒ–ç”¨æˆ·ï¼Œis_personalization=0æˆ–2: éä¸ªæ€§åŒ–ç”¨æˆ·
        group_ids = return_dict.get("group_ids", None)
        
        # è®¡ç®—å®Œæ•´çš„CLæŸå¤±ï¼ˆä½¿ç”¨æ”¹è¿›çš„ç‰ˆæœ¬ï¼‰
        total_loss = self.compute_cl_loss(
            base_loss=base_loss,
            feature_embeddings=feature_embeddings,
            h1_logits=h1_logits,  # ä¸ªæ€§åŒ–è§†å›¾ï¼ˆæ•™å¸ˆï¼‰
            h2_logits=h2_logits,  # éä¸ªæ€§åŒ–è§†å›¾ï¼ˆå­¦ç”Ÿï¼‰
            labels=y_true,
            group_ids=group_ids  # ç»„æ ‡è¯†
        )
        
        return total_loss 