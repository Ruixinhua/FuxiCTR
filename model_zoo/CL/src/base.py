# =========================================================================
# Copyright (C) 2024. The FuxiCTR Library. All rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========================================================================

"""
å¯¹æ¯”å­¦ä¹ åŸºç¡€ç»„ä»¶

æä¾›é€šç”¨çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š
1. ç‰¹å¾æ©ç  (Feature Masking)
2. ç‰¹å¾å¯¹é½æŸå¤± (Feature Alignment Loss) 
3. å­—æ®µå‡åŒ€æ€§æŸå¤± (Field Uniformity Loss)
4. è·ç¦»æŸå¤± (Distance Loss)
"""

import torch
import torch.nn.functional as F
from abc import ABC, abstractmethod
import logging



class ContrastiveLearningBase(ABC):
    """
    å¯¹æ¯”å­¦ä¹ åŸºç¡€ç±»
    
    æä¾›æ‰€æœ‰CLæ¨¡å‹å…±äº«çš„åŸºç¡€åŠŸèƒ½ï¼š
    - ç‰¹å¾æ©ç ç”Ÿæˆ
    - å„ç§CLæŸå¤±è®¡ç®—
    - ç»Ÿä¸€çš„CLé…ç½®æ¥å£
    """
    
    def __init__(self, cl_config=None, **kwargs):
        """
        åˆå§‹åŒ–ContrastiveLearningBase
        
        Args:
            cl_config (dict): CLé…ç½®å‚æ•° (ä¼ ç»Ÿæ–¹å¼)
            **kwargs: æ”¯æŒä»é¡¶çº§å‚æ•°è¯»å–CLé…ç½® (autotunerå…¼å®¹)
        """
        # ğŸ”§ æ”¯æŒautotunerçš„æ‰å¹³åŒ–å‚æ•°ç»“æ„
        # å¦‚æœkwargsä¸­åŒ…å«CLå‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼›å¦åˆ™ä½¿ç”¨cl_configå­—å…¸
        self.cl_config = cl_config or {}
        
        # ğŸ¯ å‚æ•°è¯»å–ä¼˜å…ˆçº§ï¼škwargs > cl_config > default
        def get_param(key, default_value):
            return kwargs.get(key, self.cl_config.get(key, default_value))
        
        self.personalization_feature_list = get_param("personalization_feature_list", [])

        # ğŸ”§ å‘åå…¼å®¹æ€§
        self.use_personalisation = True if len(self.personalization_feature_list) else False

        if len(self.personalization_feature_list) == 0 and self.use_personalisation:
            logging.warning("personalization_feature_listä¸ºç©ºï¼Œä½†use_personalisation=True")

        
        self.mask_type = get_param('mask_type', 'Personalisation')
        self.use_cl_mask = get_param('use_cl_mask', False)
        self.keep_prob = get_param('keep_prob', 1.0)
        
        # ğŸ”§ æŸå¤±æƒé‡å‚æ•°
        self.base_loss_weight = get_param('base_loss_weight', 1.0)
        self.feature_alignment_loss_weight = get_param('feature_alignment_loss_weight', 0.0)
        self.field_uniformity_loss_weight = get_param('field_uniformity_loss_weight', 0.0)
        self.distance_loss_weight = get_param('distance_loss_weight', 0.0)
        
        # ğŸ”§ å†…å­˜ä¼˜åŒ–å‚æ•°
        self.max_pairs_for_alignment = get_param('max_pairs_for_alignment', 50000)
        self.chunk_size_for_alignment = get_param('chunk_size_for_alignment', 256)
        
        # ğŸš€ æ–°å¢çš„CLæŸå¤±ç±»å‹å‚æ•°
        self.knowledge_distillation_loss_weight = get_param('knowledge_distillation_loss_weight', 0.0)
        self.group_aware_loss_weight = get_param('group_aware_loss_weight', 0.0)
        self.mask_strategy = get_param('mask_strategy', 'zero')  # 'zero', 'noise', 'dropout'
        self.mask_noise_std = get_param('mask_noise_std', 0.1)
        self.mask_dropout_rate = get_param('mask_dropout_rate', 0.3)
        self.temperature = get_param('temperature', 4.0)  # çŸ¥è¯†è’¸é¦æ¸©åº¦å‚æ•°

        self.use_cl_loss = (self.feature_alignment_loss_weight > 0 or
                            self.field_uniformity_loss_weight > 0 or
                            self.distance_loss_weight > 0 or
                            self.knowledge_distillation_loss_weight > 0 or
                            self.group_aware_loss_weight > 0)
        logging.info(f"Use CL Loss: {self.use_cl_loss}")
        # æŸå¤±ç¼“å­˜
        self.feature_alignment_loss = None
        self.field_uniformity_loss = None
        self.distance_loss = None
        self.knowledge_distillation_loss = None
        self.group_aware_loss = None
        
        if self.use_personalisation:
            self._setup_personalization()
            
    def _setup_personalization(self):
        """è®¾ç½®ä¸ªæ€§åŒ–ç›¸å…³å‚æ•°"""
        if not self.personalization_feature_list:
            logging.warning("personalization_feature_listä¸ºç©ºï¼Œä½†use_personalisation=True")

    def get_feature_embeddings(self, embedding_layer, X, feature_names=None):
        """
        è·å–å•ä¸ªç‰¹å¾çš„åµŒå…¥è¡¨ç¤º
        
        Args:
            embedding_layer: åµŒå…¥å±‚
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            
        Returns:
            dict: {feature_name: embedding_tensor}
        """
        feature_embeddings = {}
        
        if feature_names is None:
            feature_names = list(X.keys())
            
        for feature in feature_names:
            if feature in X:
                # ä¸ºå•ä¸ªç‰¹å¾åˆ›å»ºè¾“å…¥
                single_feature_input = {feature: X[feature]}
                # è·å–è¯¥ç‰¹å¾çš„åµŒå…¥
                feature_emb = embedding_layer(single_feature_input)
                feature_embeddings[feature] = feature_emb
                
        return feature_embeddings
    
    def sum_unique_pairwise_distances(self, tensor):
        """
        è®¡ç®—å¼ é‡ä¸­æ‰€æœ‰å”¯ä¸€æˆå¯¹å…ƒç´ çš„L2è·ç¦»ä¹‹å’Œ (å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬)
        
        Args:
            tensor: è¾“å…¥å¼ é‡ [batch_size, feature_dim]
            
        Returns:
            tuple: (sum_distances, n_pairs)
        """
        batch_size = tensor.size(0)
        
        if batch_size <= 1:
            return torch.tensor(0.0, device=tensor.device, dtype=tensor.dtype), \
                   torch.tensor(0.0, device=tensor.device, dtype=tensor.dtype)
        
        # å†…å­˜ä¼˜åŒ–å‚æ•°
        max_pairs = getattr(self, 'max_pairs_for_alignment', 50000)  # æœ€å¤§æˆå¯¹æ•°é‡
        chunk_size = getattr(self, 'chunk_size_for_alignment', 256)   # åˆ†å—å¤§å°
        
        # è®¡ç®—æ€»çš„æˆå¯¹æ•°é‡
        total_pairs = batch_size * (batch_size - 1) // 2
        
        # å¦‚æœæˆå¯¹æ•°é‡è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨é‡‡æ ·ç­–ç•¥
        if total_pairs > max_pairs:
            return self._compute_sampled_pairwise_distances(tensor, max_pairs)
        
        # å¦‚æœbatch_sizeè¾ƒå¤§ï¼Œä½¿ç”¨åˆ†å—è®¡ç®—
        if batch_size > chunk_size:
            return self._compute_chunked_pairwise_distances(tensor, chunk_size)
        
        # åŸå§‹è®¡ç®—æ–¹æ³•ï¼ˆé€‚ç”¨äºå°batchï¼‰
        return self._compute_full_pairwise_distances(tensor)
    
    def _compute_full_pairwise_distances(self, tensor):
        """åŸå§‹çš„å®Œæ•´æˆå¯¹è·ç¦»è®¡ç®—"""
        batch_size = tensor.size(0)
        n_pairs = batch_size * (batch_size - 1) // 2
        
        # åˆ›å»ºä¸Šä¸‰è§’æ©ç 
        i, j = torch.meshgrid(torch.arange(batch_size, device=tensor.device), 
                             torch.arange(batch_size, device=tensor.device), indexing='ij')
        mask = i < j
        
        # è·å–å”¯ä¸€å¯¹
        elements_i = tensor[i[mask]]
        elements_j = tensor[j[mask]]
        
        # è®¡ç®—L2è·ç¦»
        distances = torch.norm(elements_i - elements_j, dim=-1)
        sum_distances = torch.sum(distances)
        
        return sum_distances, torch.tensor(n_pairs, device=tensor.device, dtype=tensor.dtype)
    
    def _compute_sampled_pairwise_distances(self, tensor, max_pairs):
        """é‡‡æ ·ç­–ç•¥çš„æˆå¯¹è·ç¦»è®¡ç®—"""
        batch_size = tensor.size(0)
        
        # éšæœºé‡‡æ ·æˆå¯¹ç´¢å¼•
        all_pairs = []
        for i in range(batch_size):
            for j in range(i + 1, batch_size):
                all_pairs.append((i, j))
        
        # éšæœºé€‰æ‹©max_pairsä¸ªæˆå¯¹
        import random
        if len(all_pairs) > max_pairs:
            sampled_pairs = random.sample(all_pairs, max_pairs)
        else:
            sampled_pairs = all_pairs
        
        # è®¡ç®—é‡‡æ ·æˆå¯¹çš„è·ç¦»
        sum_distances = 0.0
        for i, j in sampled_pairs:
            distance = torch.norm(tensor[i] - tensor[j])
            sum_distances += distance
        
        return sum_distances, torch.tensor(len(sampled_pairs), device=tensor.device, dtype=tensor.dtype)
    
    def _compute_chunked_pairwise_distances(self, tensor, chunk_size):
        """åˆ†å—è®¡ç®—çš„æˆå¯¹è·ç¦»è®¡ç®—"""
        batch_size = tensor.size(0)
        total_sum = 0.0
        total_pairs = 0
        
        # åˆ†å—å¤„ç†
        for start_i in range(0, batch_size, chunk_size):
            end_i = min(start_i + chunk_size, batch_size)
            chunk_i = tensor[start_i:end_i]
            
            for start_j in range(start_i, batch_size, chunk_size):
                end_j = min(start_j + chunk_size, batch_size)
                chunk_j = tensor[start_j:end_j]
                
                # è®¡ç®—chunké—´çš„è·ç¦»
                chunk_sum, chunk_pairs = self._compute_chunk_distances(
                    chunk_i, chunk_j, start_i, start_j, end_i, end_j
                )
                total_sum += chunk_sum
                total_pairs += chunk_pairs
        
        return total_sum, torch.tensor(total_pairs, device=tensor.device, dtype=tensor.dtype)
    
    def _compute_chunk_distances(self, chunk_i, chunk_j, start_i, start_j, end_i, end_j):
        """è®¡ç®—ä¸¤ä¸ªchunkä¹‹é—´çš„è·ç¦»"""
        sum_distances = 0.0
        pairs_count = 0
        
        for i, emb_i in enumerate(chunk_i):
            global_i = start_i + i
            j_start = max(0, start_j - start_i) if start_i == start_j else 0
            j_start = max(j_start, i + 1) if start_i == start_j else j_start
            
            for j in range(j_start, len(chunk_j)):
                global_j = start_j + j
                if global_i < global_j:  # åªè®¡ç®—ä¸Šä¸‰è§’
                    distance = torch.norm(emb_i - chunk_j[j])
                    sum_distances += distance
                    pairs_count += 1
        
        return sum_distances, pairs_count
    
    def compute_feature_alignment_loss(self, feature_embeddings):
        """
        è®¡ç®—ç‰¹å¾å¯¹é½æŸå¤±
        
        Args:
            feature_embeddings: {feature_name: embedding_tensor}
            
        Returns:
            torch.Tensor: ç‰¹å¾å¯¹é½æŸå¤±
        """
        total_distance = 0.0
        total_pairs = 0.0
        
        for feature_name, feature_emb in feature_embeddings.items():
            # feature_emb shape: [batch_size, embedding_dim]
            if feature_emb.dim() > 2:
                feature_emb = feature_emb.view(feature_emb.size(0), -1)
                
            sum_distances, n_pairs = self.sum_unique_pairwise_distances(feature_emb)
            total_distance += sum_distances
            total_pairs += n_pairs
        
        # é¿å…é™¤é›¶
        if total_pairs > 0:
            feature_alignment_loss = total_distance / total_pairs
        else:
            feature_alignment_loss = torch.tensor(0.0, device=feature_emb.device)
            
        return feature_alignment_loss
    
    def compute_field_uniformity_loss(self, feature_embeddings):
        """
        è®¡ç®—å­—æ®µå‡åŒ€æ€§æŸå¤± (ä¿®å¤ç‰ˆæœ¬)
        
        é€šè¿‡æœ€å°åŒ–ä¸åŒç‰¹å¾é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥ä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§
        
        Args:
            feature_embeddings: {feature_name: embedding_tensor}
            
        Returns:
            torch.Tensor: å­—æ®µå‡åŒ€æ€§æŸå¤±
        """
        if not feature_embeddings or len(feature_embeddings) < 2:
            return torch.tensor(0.0, dtype=torch.float32)
        
        # æ ‡å‡†åŒ–ç‰¹å¾å‘é‡
        normalized_features = {}
        for feature_name, feature_emb in feature_embeddings.items():
            # feature_emb shape: [batch_size, embedding_dim]
            if feature_emb.dim() > 2:
                feature_emb = feature_emb.view(feature_emb.size(0), -1)
            normalized_features[feature_name] = F.normalize(feature_emb, p=2, dim=-1)
        
        # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        feature_cos_sim_list = []
        feature_names = list(normalized_features.keys())
        
        for i, feature_i in enumerate(feature_names):
            for j, feature_j in enumerate(feature_names):
                if i < j:  # åªè®¡ç®—ä¸Šä¸‰è§’ï¼Œé¿å…é‡å¤
                    # ğŸ”§ ä¿®å¤ï¼šæŒ‰æ ·æœ¬è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç„¶åå–batchå¹³å‡
                    cos_sim_per_sample = torch.sum(
                        normalized_features[feature_i] * normalized_features[feature_j], 
                        dim=-1  # æ²¿ç‰¹å¾ç»´åº¦æ±‚å’Œï¼Œä¿æŒbatchç»´åº¦
                    )
                    # å–batchå¹³å‡çš„ç»å¯¹å€¼ï¼ˆæˆ‘ä»¬å¸Œæœ›ç›¸ä¼¼åº¦å°½å¯èƒ½å°ï¼‰
                    avg_cos_sim = torch.mean(torch.abs(cos_sim_per_sample))
                    feature_cos_sim_list.append(avg_cos_sim)
        
        if feature_cos_sim_list:
            # å­—æ®µå‡åŒ€æ€§æŸå¤±ï¼šç‰¹å¾é—´ç›¸ä¼¼åº¦çš„å¹³å‡å€¼
            field_uniformity_loss = torch.mean(torch.stack(feature_cos_sim_list))
        else:
            field_uniformity_loss = torch.tensor(0.0, dtype=torch.float32)
            
        return field_uniformity_loss

    def compute_distance_loss(self, h1_logits, h2_logits, labels):
        """
        è®¡ç®—è·ç¦»æŸå¤± (ä¿®å¤ç‰ˆæœ¬)
        
        ä½¿ç”¨å¯¹æ¯”å­¦ä¹ çš„æ€æƒ³ï¼šç›¸åŒæ ‡ç­¾çš„h1å’Œh2åº”è¯¥ç›¸è¿‘ï¼Œä¸åŒæ ‡ç­¾çš„åº”è¯¥è¿œç¦»
        
        Args:
            h1_logits: ç¬¬ä¸€ä¸ªè§†å›¾çš„logits
            h2_logits: ç¬¬äºŒä¸ªè§†å›¾çš„logits  
            labels: çœŸå®æ ‡ç­¾
            
        Returns:
            torch.Tensor: è·ç¦»æŸå¤±
        """
        if h1_logits is None or h2_logits is None:
            return torch.tensor(0.0, dtype=torch.float32)
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç®€å•çš„MSEæŸå¤±é¼“åŠ±ä¸¤ä¸ªè§†å›¾çš„ä¸€è‡´æ€§
        # å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ï¼šç›¸åŒæ ·æœ¬çš„ä¸åŒè§†å›¾åº”è¯¥äº§ç”Ÿç›¸ä¼¼çš„é¢„æµ‹
        distance_loss = F.mse_loss(h1_logits, h2_logits, reduction='mean')
        
        return distance_loss
    
    def compute_knowledge_distillation_loss(self, h1_logits, h2_logits, labels):
        """
        è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤± (æ ¸å¿ƒæ”¹è¿›)
        
        è®©éä¸ªæ€§åŒ–è§†å›¾(h2)ä»ä¸ªæ€§åŒ–è§†å›¾(h1)ä¸­å­¦ä¹ è½¯æ ‡ç­¾çŸ¥è¯†
        è¿™æ˜¯ä¸“é—¨ä¸ºæå‡éä¸ªæ€§åŒ–ç”¨æˆ·æ€§èƒ½è®¾è®¡çš„æŸå¤±
        
        Args:
            h1_logits: ä¸ªæ€§åŒ–è§†å›¾çš„logits (æ•™å¸ˆ)
            h2_logits: éä¸ªæ€§åŒ–è§†å›¾çš„logits (å­¦ç”Ÿ)  
            labels: çœŸå®æ ‡ç­¾
            
        Returns:
            torch.Tensor: çŸ¥è¯†è’¸é¦æŸå¤±
        """
        if h1_logits is None or h2_logits is None:
            return torch.tensor(0.0, dtype=torch.float32)
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†äºŒåˆ†ç±»æƒ…å†µï¼Œå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
        if h1_logits.shape[-1] == 1:
            # äºŒåˆ†ç±»ï¼šä½¿ç”¨sigmoidè½¬æ¢ä¸ºæ¦‚ç‡
            eps = 1e-7  # å¢å¤§epsilonå€¼
            teacher_probs = torch.clamp(torch.sigmoid(h1_logits.squeeze() / self.temperature), eps, 1-eps)
            student_probs = torch.clamp(torch.sigmoid(h2_logits.squeeze() / self.temperature), eps, 1-eps)
            
            # æ„é€ å®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒ [p_negative, p_positive]
            teacher_probs_full = torch.stack([1 - teacher_probs, teacher_probs], dim=-1)
            student_log_probs_full = torch.stack([
                torch.log(1 - student_probs + 1e-8), 
                torch.log(student_probs + 1e-8)
            ], dim=-1)
            
            # KLæ•£åº¦æŸå¤±
            kd_loss = F.kl_div(student_log_probs_full, teacher_probs_full, reduction='batchmean')
        else:
            # å¤šåˆ†ç±»ï¼šä½¿ç”¨åŸæœ‰é€»è¾‘
            teacher_probs = F.softmax(h1_logits / self.temperature, dim=-1)
            student_log_probs = F.log_softmax(h2_logits / self.temperature, dim=-1)
            kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')
        
        # æ¸©åº¦å¹³æ–¹ç¼©æ”¾ï¼ˆæ ‡å‡†KDåšæ³•ï¼‰
        kd_loss = kd_loss * (self.temperature ** 2)
        
        return kd_loss
    
    def compute_group_aware_loss(self, h1_logits, h2_logits, labels, group_ids=None):
        """
        è®¡ç®—ç»„æ„ŸçŸ¥æŸå¤± (é’ˆå¯¹éä¸ªæ€§åŒ–ç”¨æˆ·çš„ä¸“é—¨ä¼˜åŒ–)
        
        ä¸“é—¨ä¼˜åŒ–éä¸ªæ€§åŒ–ç”¨æˆ·(group_2.0)çš„é¢„æµ‹æ€§èƒ½
        
        Args:
            h1_logits: ä¸ªæ€§åŒ–è§†å›¾çš„logits
            h2_logits: éä¸ªæ€§åŒ–è§†å›¾çš„logits
            labels: çœŸå®æ ‡ç­¾
            group_ids: ç»„æ ‡è¯† (1.0=ä¸ªæ€§åŒ–ç”¨æˆ·, 2.0=éä¸ªæ€§åŒ–ç”¨æˆ·)
            
        Returns:
            torch.Tensor: ç»„æ„ŸçŸ¥æŸå¤±
        """
        if h1_logits is None or h2_logits is None:
            return torch.tensor(0.0, dtype=torch.float32)

        # å¦‚æœæ²¡æœ‰ç»„ä¿¡æ¯ï¼Œå‡è®¾æ‰€æœ‰æ ·æœ¬éƒ½éœ€è¦ä¼˜åŒ–éä¸ªæ€§åŒ–æ€§èƒ½
        if group_ids is None:
            # ğŸ”§ ä¿®å¤ç»´åº¦ä¸åŒ¹é…ï¼šç¡®ä¿logitså’Œlabelsç»´åº¦ä¸€è‡´
            # å¯¹æ‰€æœ‰æ ·æœ¬ä½¿ç”¨éä¸ªæ€§åŒ–è§†å›¾çš„BCEæŸå¤±
            if h2_logits.dim() > 1 and h2_logits.shape[-1] == 1:
                h2_logits_flat = h2_logits.squeeze(-1)  # [batch_size, 1] -> [batch_size]
            else:
                h2_logits_flat = h2_logits
                
            if labels.dim() > 1 and labels.shape[-1] == 1:
                labels_flat = labels.squeeze(-1)  # [batch_size, 1] -> [batch_size]
            else:
                labels_flat = labels
                
            group_loss = F.binary_cross_entropy_with_logits(h2_logits_flat, labels_flat.float(), reduction='mean')
        else:
            # åªå¯¹éä¸ªæ€§åŒ–ç”¨æˆ·(group_2.0)ä¼˜åŒ–
            non_personalized_mask = (group_ids == 2.0)
            num_non_personalized = non_personalized_mask.sum().item()

            if num_non_personalized > 0:
                non_pers_h2_logits = h2_logits[non_personalized_mask]
                non_pers_labels = labels[non_personalized_mask]
                
                # ğŸ”§ ä¿®å¤ç»´åº¦ä¸åŒ¹é…ï¼šç¡®ä¿logitså’Œlabelsç»´åº¦ä¸€è‡´
                if non_pers_h2_logits.dim() > 1 and non_pers_h2_logits.shape[-1] == 1:
                    non_pers_h2_logits_flat = non_pers_h2_logits.squeeze(-1)
                else:
                    non_pers_h2_logits_flat = non_pers_h2_logits
                    
                if non_pers_labels.dim() > 1 and non_pers_labels.shape[-1] == 1:
                    non_pers_labels_flat = non_pers_labels.squeeze(-1)
                else:
                    non_pers_labels_flat = non_pers_labels
                
                group_loss = F.binary_cross_entropy_with_logits(
                    non_pers_h2_logits_flat, 
                    non_pers_labels_flat.float(), 
                    reduction='mean'
                )
            else:
                group_loss = torch.tensor(0.0, dtype=torch.float32)

        return group_loss
    
    def compute_cl_loss(self, base_loss, feature_embeddings=None, h1_logits=None, h2_logits=None, labels=None, group_ids=None):
        """
        è®¡ç®—å®Œæ•´çš„å¯¹æ¯”å­¦ä¹ æŸå¤± (æ”¹è¿›ç‰ˆæœ¬)
        
        æ–°å¢é’ˆå¯¹éä¸ªæ€§åŒ–ç”¨æˆ·çš„ä¸“é—¨ä¼˜åŒ–ç­–ç•¥
        
        Args:
            base_loss: åŸºç¡€æŸå¤±
            feature_embeddings: ç‰¹å¾åµŒå…¥å­—å…¸ 
            h1_logits: ç¬¬ä¸€ä¸ªè§†å›¾logits (ä¸ªæ€§åŒ–è§†å›¾ - æ•™å¸ˆ)
            h2_logits: ç¬¬äºŒä¸ªè§†å›¾logits (éä¸ªæ€§åŒ–è§†å›¾ - å­¦ç”Ÿ)
            labels: çœŸå®æ ‡ç­¾
            group_ids: ç»„æ ‡è¯† (1.0=ä¸ªæ€§åŒ–ç”¨æˆ·, 2.0=éä¸ªæ€§åŒ–ç”¨æˆ·)
            
        Returns:
            torch.Tensor: æ€»æŸå¤±
        """
        total_loss = self.base_loss_weight * base_loss

        # ğŸ”§ æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(base_loss) or torch.isinf(base_loss):
            logging.warning(f"åŸºç¡€æŸå¤±å¼‚å¸¸: {base_loss}")
            base_loss = torch.tensor(0.0, dtype=base_loss.dtype, device=base_loss.device)
        
        # ğŸ¯ æ–°å¢ï¼šçŸ¥è¯†è’¸é¦æŸå¤± (æ ¸å¿ƒæ”¹è¿›)
        if self.knowledge_distillation_loss_weight > 0 and h1_logits is not None and h2_logits is not None:
            # åœ¨çŸ¥è¯†è’¸é¦æŸå¤±è®¡ç®—å‰æ£€æŸ¥logitsæ˜¯å¦åŒ…å«NaNæˆ–Inf
            if torch.isnan(h1_logits).any() or torch.isinf(h1_logits).any():
                logging.warning(f"h1_logitsåŒ…å«NaNæˆ–Infï¼Œè·³è¿‡çŸ¥è¯†è’¸é¦æŸå¤±è®¡ç®—ã€‚")
                self.knowledge_distillation_loss = torch.tensor(0.0, dtype=torch.float32, device=h1_logits.device)
            else:
                self.knowledge_distillation_loss = self.compute_knowledge_distillation_loss(h1_logits, h2_logits, labels)
                # æ•°å€¼æ£€æŸ¥
                if torch.isnan(self.knowledge_distillation_loss) or torch.isinf(self.knowledge_distillation_loss):
                    logging.warning(f"çŸ¥è¯†è’¸é¦æŸå¤±å¼‚å¸¸: {self.knowledge_distillation_loss}")
                    self.knowledge_distillation_loss = torch.tensor(0.0, dtype=self.knowledge_distillation_loss.dtype, device=self.knowledge_distillation_loss.device)
            weighted_kd_loss = self.knowledge_distillation_loss_weight * self.knowledge_distillation_loss
            total_loss += weighted_kd_loss

        # ğŸ¯ æ–°å¢ï¼šç»„æ„ŸçŸ¥æŸå¤± (ä¸“é—¨ä¼˜åŒ–éä¸ªæ€§åŒ–ç”¨æˆ·)
        if self.group_aware_loss_weight > 0 and h1_logits is not None and h2_logits is not None:
            self.group_aware_loss = self.compute_group_aware_loss(h1_logits, h2_logits, labels, group_ids)
            # æ•°å€¼æ£€æŸ¥
            if torch.isnan(self.group_aware_loss) or torch.isinf(self.group_aware_loss):
                logging.warning(f"ç»„æ„ŸçŸ¥æŸå¤±å¼‚å¸¸: {self.group_aware_loss}")
                self.group_aware_loss = torch.tensor(0.0, dtype=self.group_aware_loss.dtype, device=self.group_aware_loss.device)
            weighted_group_loss = self.group_aware_loss_weight * self.group_aware_loss
            total_loss += weighted_group_loss

        # ç‰¹å¾å¯¹é½æŸå¤±
        if self.feature_alignment_loss_weight > 0 and feature_embeddings is not None:
            self.feature_alignment_loss = self.compute_feature_alignment_loss(feature_embeddings)
            # æ•°å€¼æ£€æŸ¥
            if torch.isnan(self.feature_alignment_loss) or torch.isinf(self.feature_alignment_loss):
                logging.warning(f"ç‰¹å¾å¯¹é½æŸå¤±å¼‚å¸¸: {self.feature_alignment_loss}")
                self.feature_alignment_loss = torch.tensor(0.0, dtype=self.feature_alignment_loss.dtype, device=self.feature_alignment_loss.device)
            weighted_fa_loss = self.feature_alignment_loss_weight * self.feature_alignment_loss
            total_loss += weighted_fa_loss

        # å­—æ®µå‡åŒ€æ€§æŸå¤±  
        if self.field_uniformity_loss_weight > 0 and feature_embeddings is not None:
            self.field_uniformity_loss = self.compute_field_uniformity_loss(feature_embeddings)
            # æ•°å€¼æ£€æŸ¥
            if torch.isnan(self.field_uniformity_loss) or torch.isinf(self.field_uniformity_loss):
                logging.warning(f"å­—æ®µå‡åŒ€æ€§æŸå¤±å¼‚å¸¸: {self.field_uniformity_loss}")
                self.field_uniformity_loss = torch.tensor(0.0, dtype=self.field_uniformity_loss.dtype, device=self.field_uniformity_loss.device)
            weighted_fu_loss = self.field_uniformity_loss_weight * self.field_uniformity_loss
            total_loss += weighted_fu_loss

        # è·ç¦»æŸå¤± (åŸæœ‰çš„ï¼Œç°åœ¨ä½œä¸ºå¤‡é€‰)
        if self.distance_loss_weight > 0 and h1_logits is not None and h2_logits is not None:
            self.distance_loss = self.compute_distance_loss(h1_logits, h2_logits, labels)
            # æ•°å€¼æ£€æŸ¥
            if torch.isnan(self.distance_loss) or torch.isinf(self.distance_loss):
                logging.warning(f"è·ç¦»æŸå¤±å¼‚å¸¸: {self.distance_loss}")
                self.distance_loss = torch.tensor(0.0, dtype=self.distance_loss.dtype, device=self.distance_loss.device)
            weighted_dist_loss = self.distance_loss_weight * self.distance_loss
            total_loss += weighted_dist_loss

        # ğŸ”§ æœ€ç»ˆæ•°å€¼æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logging.error(f"æ€»æŸå¤±å¼‚å¸¸: {total_loss}, åŸºç¡€æŸå¤±: {base_loss}")
            total_loss = base_loss  # å›é€€åˆ°åŸºç¡€æŸå¤±
        
        return total_loss
    
    def get_group_ids(self, inputs):
        """
        ä»inputsä¸­æå–ç»„æ ‡è¯†ä¿¡æ¯
        
        æ ¹æ®is_personalizationç‰¹å¾åŒºåˆ†ä¸ªæ€§åŒ–/éä¸ªæ€§åŒ–ç”¨æˆ·ï¼š
        - is_personalization=1: ä¸ªæ€§åŒ–ç”¨æˆ· (è¿”å›1.0)
        - is_personalization=0æˆ–2: éä¸ªæ€§åŒ–ç”¨æˆ· (è¿”å›2.0)
        
        Args:
            inputs: æ¨¡å‹è¾“å…¥å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰ç‰¹å¾
            
        Returns:
            torch.Tensor or None: ç»„æ ‡è¯†å¼ é‡ï¼Œ1.0è¡¨ç¤ºä¸ªæ€§åŒ–ç”¨æˆ·ï¼Œ2.0è¡¨ç¤ºéä¸ªæ€§åŒ–ç”¨æˆ·
        """
        try:
            if 'is_personalization' in inputs:
                personalization_flag = inputs['is_personalization']
                
                # ç¡®ä¿æ˜¯å¼ é‡æ ¼å¼
                if not isinstance(personalization_flag, torch.Tensor):
                    personalization_flag = torch.tensor(personalization_flag)
                
                # è½¬æ¢ä¸ºç»„æ ‡è¯†ï¼š1->1.0 (ä¸ªæ€§åŒ–), 0æˆ–2->2.0 (éä¸ªæ€§åŒ–)
                group_ids = torch.where(
                    personalization_flag == 1.0,
                    torch.tensor(1.0, dtype=torch.float32, device=personalization_flag.device),  # ä¸ªæ€§åŒ–ç”¨æˆ·
                    torch.tensor(2.0, dtype=torch.float32, device=personalization_flag.device)   # éä¸ªæ€§åŒ–ç”¨æˆ·
                )
                
                return group_ids
            else:
                return None
                
        except Exception as e:
            logging.warning(f"æå–ç»„æ ‡è¯†æ—¶å‡ºé”™: {e}")
            return None
