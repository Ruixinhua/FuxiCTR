# =========================================================================
# Copyright (C) 2024. The FuxiCTR Library. All rights reserved.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========================================================================

"""
MaskNetCL: MaskNet with Contrastive Learning

åŸºäºMaskNetWithMultiToweræ¨¡å‹å®ç°çš„å¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬ï¼Œæ”¯æŒï¼š
1. ä¸ªæ€§åŒ–ç‰¹å¾æ©ç 
2. ç‰¹å¾å¯¹é½æŸå¤±  
3. å­—æ®µå‡åŒ€æ€§æŸå¤±
4. è·ç¦»æŸå¤±
5. SerialMaskNet/ParallelMaskNetç»“æ„
6. ğŸ¯ å¤šå¡”(MT)æ”¯æŒï¼ˆé€šè¿‡ç»§æ‰¿MaskNetWithMultiTowerå®ç°ï¼‰
"""

import torch
import torch.nn as nn
import logging

# å¯¼å…¥MaskNetWithMultiToweræ¨¡å‹
from ...MaskNet.src.MaskNet import MaskNet
from .base import ContrastiveLearningBase
from fuxictr.pytorch.layers import MultiTowerModule


class MaskNetCL(MaskNet, ContrastiveLearningBase):
    """
    MaskNet with Contrastive Learning
    
    ç»§æ‰¿è‡ªMaskNetWithMultiTowerï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ åŠŸèƒ½
    æ”¯æŒSerialMaskNetå’ŒParallelMaskNetä¸¤ç§ç»“æ„
    ğŸ¯ é€šè¿‡çˆ¶ç±»æ”¯æŒå¤šå¡”(MT)ç»“æ„
    """
    
    def __init__(self, 
                 feature_map,
                 learning_rate=1e-3,
                 # CLç›¸å…³å‚æ•°
                 cl_config=None,
                 # MTç›¸å…³å‚æ•°ï¼ˆç»§æ‰¿è‡ªMaskNetWithMultiTowerï¼‰
                 use_domain_aware_structure=False,
                 tower_hidden_units_list=None,
                 tower_activation='ReLU',
                 tower_l2_reg_list=None,
                 tower_dropout_list=None,
                 use_bn_tower=True,
                 scene_name='scene_id',
                 scene_num_shift=1,
                 use_scene_id_mapping=False,
                 mapping_feature_name=None,
                 mapping_feature_type=None,
                 feature2id_dict=None,
                 default_value=None,
                 feature_map_dict=None,
                 **kwargs):
        
        # åˆå§‹åŒ–ContrastiveLearningBase
        ContrastiveLearningBase.__init__(self, cl_config=cl_config, **kwargs)
        MaskNet.__init__(self, feature_map=feature_map, **kwargs)
        # åˆå§‹åŒ–MaskNetWithMultiTowerï¼ˆåŒ…å«æ‰€æœ‰MTç›¸å…³åŠŸèƒ½ï¼‰
        self.use_domain_aware_structure = use_domain_aware_structure
        multi_tower_params = {
            'tower_hidden_units_list': tower_hidden_units_list,
            'tower_activation': tower_activation,
            'tower_l2_reg_list': tower_l2_reg_list,
            'tower_dropout_list': tower_dropout_list,
            'use_bn_tower': use_bn_tower,
            'scene_name': scene_name,
            'scene_num_shift': scene_num_shift,
            'use_scene_id_mapping': use_scene_id_mapping,
            'mapping_feature_name': mapping_feature_name,
            'mapping_feature_type': mapping_feature_type,
            'feature2id_dict': feature2id_dict,
            'default_value': default_value,
            'feature_map_dict': feature_map_dict
        }
        # å­˜å‚¨å‚æ•°ç”¨äºåŸŸæ„ŸçŸ¥ç»“æ„åˆå§‹åŒ–
        if use_domain_aware_structure:
            self._masknet_params = {
                'input_dim': feature_map.num_fields * kwargs.get('embedding_dim', 16),
                'hidden_units': kwargs.get('dnn_hidden_units', [256, 128, 64])
            }
            # ä¼ é€’é¢å¤–å‚æ•°ç»™å¤šå¡”åˆå§‹åŒ–æ–¹æ³•
            self._init_multi_tower_structure(multi_tower_params, kwargs)
            # ç¡®ä¿MultiTowerModuleåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.multi_tower_module.to(self.device)
        logging.info(f"MaskNetWithMultiTower initialized. Use domain-aware structure: {use_domain_aware_structure}")
        self.compile(kwargs["optimizer"], kwargs["loss"], learning_rate)
        self.reset_parameters()
        self.model_to_device()

    def _init_multi_tower_structure(self, multi_tower_params, model_kwargs):
        """åˆå§‹åŒ–å¤šå¡”ç»“æ„ï¼Œä»¿ç…§ DCNv3 çš„å®ç°"""
        tower_hidden_units_list = multi_tower_params['tower_hidden_units_list']
        if tower_hidden_units_list is None:
            raise ValueError("`tower_hidden_units_list` must be specified when using multi-tower structure.")

        # è®¡ç®— MaskNet è¾“å‡ºç»´åº¦ï¼Œæ ¹æ®ä¸åŒç±»å‹çš„MaskNetè®¡ç®—
        dnn_hidden_units = self._masknet_params['hidden_units']
        masknet_input_dim = self._masknet_params['input_dim']

        # æ ¹æ®mask_netçš„ç±»å‹è®¡ç®—æ­£ç¡®çš„è¾“å…¥ç»´åº¦
        if hasattr(self.mask_net, 'hidden_units'):
            # SerialMaskNet - è¾“å‡ºç»´åº¦æ˜¯æœ€åä¸€ä¸ªéšè—å±‚çš„ç»´åº¦
            if dnn_hidden_units:
                tower_input_dim = dnn_hidden_units[-1]
            else:
                tower_input_dim = masknet_input_dim
        elif hasattr(self.mask_net, 'num_blocks'):
            # ParallelMaskNet - è¾“å‡ºç»´åº¦æ˜¯ block_dim * num_blocks
            # ä»ParallelMaskNetçš„æ„é€ å¯çŸ¥ï¼Œæ¯ä¸ªMaskBlockçš„è¾“å‡ºç»´åº¦æ˜¯block_dim
            # æˆ‘ä»¬éœ€è¦ä»MaskNetçš„åˆå§‹åŒ–å‚æ•°ä¸­è·å–block_dim
            block_dim = model_kwargs.get('parallel_block_dim', 64)  # é»˜è®¤å€¼64
            tower_input_dim = block_dim * self.mask_net.num_blocks
        else:
            # é»˜è®¤æƒ…å†µ
            if dnn_hidden_units:
                tower_input_dim = dnn_hidden_units[-1]
            else:
                tower_input_dim = masknet_input_dim

        # ä½¿ç”¨ MultiTowerModule æ›¿æ¢åŸæœ‰çš„å¤šå¡”å®ç°
        self.multi_tower_module = MultiTowerModule(
            input_dim=tower_input_dim,
            tower_hidden_units_list=tower_hidden_units_list,
            tower_activation=multi_tower_params['tower_activation'],
            tower_l2_reg_list=multi_tower_params['tower_l2_reg_list'],
            tower_dropout_list=multi_tower_params['tower_dropout_list'],
            use_bn_tower=multi_tower_params['use_bn_tower'],
            scene_name=multi_tower_params['scene_name'],
            scene_num_shift=multi_tower_params['scene_num_shift'],
            use_scene_id_mapping=multi_tower_params['use_scene_id_mapping'],
            mapping_feature_name=multi_tower_params['mapping_feature_name'],
            mapping_feature_type=multi_tower_params['mapping_feature_type'],
            feature2id_dict=multi_tower_params['feature2id_dict'],
            default_value=multi_tower_params['default_value'],
            feature_map_dict=multi_tower_params['feature_map_dict']
        )

    def _forward_with_logits(self, X):
        """
        ä¸€æ¬¡æ€§è®¡ç®—logitså’Œy_predï¼Œé¿å…é‡å¤è®¡ç®—
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            tuple: (logits, y_pred)
        """
        feature_emb = self.embedding_layer(X)
        
        # åº”ç”¨åµŒå…¥å±‚å½’ä¸€åŒ–
        if self.emb_norm is not None:
            feat_list = feature_emb.chunk(self.num_fields, dim=1)
            V_hidden = torch.cat([self.emb_norm[i](feat) for i, feat in enumerate(feat_list)], dim=1)
        else:
            V_hidden = feature_emb
        
        V_emb_flat = feature_emb.flatten(start_dim=1)
        V_hidden_flat = V_hidden.flatten(start_dim=1)
        
        # è·å– MaskNet çš„è¾“å‡ºï¼ˆå·²åŒ…å«æ¿€æ´»å‡½æ•°ï¼‰
        y_pred = self.mask_net(V_emb_flat, V_hidden_flat)
        
        # ä¸ºäº†è·å–logitsï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨é‡æ–°è®¡ç®—ä¸åŒ…å«æ¿€æ´»å‡½æ•°çš„è¾“å‡º
        if hasattr(self.mask_net, 'hidden_units'):
            # SerialMaskNet
            v_out = V_hidden_flat
            for idx in range(len(self.mask_net.hidden_units) - 1):
                v_out = self.mask_net.mask_blocks[idx](V_emb_flat, v_out)
            
            # è·å–æœ€åä¸€ä¸ªLinearå±‚çš„è¾“å‡ºä½œä¸ºlogitsï¼ˆä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼‰
            if hasattr(self.mask_net, 'fc') and self.mask_net.fc is not None:
                # éå†fcçš„æ‰€æœ‰å±‚ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªLinearå±‚
                logits = v_out
                for layer in self.mask_net.fc:
                    if isinstance(layer, nn.Linear):
                        logits = layer(logits)
                        break  # ç¬¬ä¸€ä¸ªLinearå±‚å°±æ˜¯è¾“å‡ºå±‚
            else:
                logits = v_out
                
        elif hasattr(self.mask_net, 'num_blocks'):
            # ParallelMaskNet
            block_out = []
            for i in range(self.mask_net.num_blocks):
                block_out.append(self.mask_net.mask_blocks[i](V_emb_flat, V_hidden_flat))
            concat_out = torch.cat(block_out, dim=-1)
            
            # è·å–dnnä¸­æœ€åä¸€ä¸ªLinearå±‚çš„è¾“å‡ºä½œä¸ºlogits
            if hasattr(self.mask_net, 'dnn'):
                # æ‰‹åŠ¨åº”ç”¨dnnçš„æ‰€æœ‰å±‚ï¼Œç›´åˆ°æœ€åä¸€ä¸ªLinearå±‚ï¼ˆä½†ä¸åŒ…æ‹¬æ¿€æ´»å‡½æ•°ï¼‰
                logits = concat_out
                for layer in self.mask_net.dnn.mlp:
                    if isinstance(layer, nn.Linear):
                        logits = layer(logits)
                        # å¦‚æœè¿™æ˜¯è¾“å‡ºå±‚ï¼ˆè¾“å‡ºç»´åº¦ä¸º1ï¼‰ï¼Œåˆ™åœæ­¢
                        if logits.shape[-1] == 1:
                            break
                    elif not isinstance(layer, nn.Sigmoid):
                        # åº”ç”¨é™¤äº†Sigmoidä¹‹å¤–çš„æ‰€æœ‰å±‚ï¼ˆå¦‚ReLUã€Dropoutç­‰ï¼‰
                        logits = layer(logits)
            else:
                logits = concat_out
        else:
            # å…¶ä»–æƒ…å†µï¼Œå‡è®¾æ²¡æœ‰æ¿€æ´»å‡½æ•°
            logits = y_pred
        
        return logits, y_pred

    def _forward_with_logits_mt(self, X):
        """
        MTæ¨¡å¼ä¸‹ä¸€æ¬¡æ€§è®¡ç®—logitså’Œy_predï¼Œé¿å…é‡å¤è®¡ç®—
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            tuple: (logits, y_pred)
        """
        feature_emb = self.embedding_layer(X)
        
        # åº”ç”¨å±‚å½’ä¸€åŒ–ï¼ˆä¸çˆ¶ç±»ä¿æŒä¸€è‡´ï¼‰
        if self.emb_norm is not None:
            feat_list = feature_emb.chunk(self.num_fields, dim=1)
            V_hidden = torch.cat([self.emb_norm[i](feat) for i, feat in enumerate(feat_list)], dim=1)
        else:
            V_hidden = feature_emb
        
        # é€šè¿‡MaskNetå¤„ç†ï¼Œä½†ä¸åº”ç”¨æœ€ç»ˆçš„è¾“å‡ºå±‚
        masknet_features = self._get_masknet_features(feature_emb.flatten(start_dim=1), V_hidden.flatten(start_dim=1))
        
        # é€šè¿‡å¤šå¡”ç»“æ„ç”Ÿæˆ logitsï¼ˆMultiTowerModule è¾“å‡ºæœªæ¿€æ´»çš„ logitsï¼‰
        final_logits = self.multi_tower_module(masknet_features, X)
        
        # åº”ç”¨è¾“å‡ºæ¿€æ´»å‡½æ•°å¾—åˆ° y_pred
        y_pred = self.output_activation(final_logits)
        
        return final_logits, y_pred

    def _get_masknet_features(self, V_emb, V_hidden):
        """è·å– MaskNet çš„ç‰¹å¾è¾“å‡ºï¼ˆä¸åŒ…å«æœ€ç»ˆè¾“å‡ºå±‚ï¼‰"""
        # æ£€æŸ¥ mask_net çš„ç±»å‹å¹¶ç›¸åº”å¤„ç†
        if hasattr(self.mask_net, 'hidden_units'):
            # SerialMaskNet - æ‰§è¡Œæ‰€æœ‰MaskBlockä½†è·³è¿‡æœ€ç»ˆçš„fcå±‚
            v_out = V_hidden
            for idx in range(len(self.mask_net.hidden_units) - 1):
                v_out = self.mask_net.mask_blocks[idx](V_emb, v_out)
            # è¿”å›MaskBlockçš„è¾“å‡ºï¼Œä¸ç»è¿‡fcå±‚
            return v_out
        elif hasattr(self.mask_net, 'num_blocks'):
            # ParallelMaskNet - è¿”å›mask_blocksçš„è¿æ¥è¾“å‡ºï¼Œè·³è¿‡dnnå±‚
            block_out = []
            for i in range(self.mask_net.num_blocks):
                block_out.append(self.mask_net.mask_blocks[i](V_emb, V_hidden))
            concat_out = torch.cat(block_out, dim=-1)
            # ç›´æ¥è¿”å›concat_outï¼Œé¿å…è¿›å…¥dnnï¼ˆå› ä¸ºdnnä¼šé™ç»´åˆ°1ï¼‰
            return concat_out
        else:
            raise ValueError(f"Unsupported mask_net type: {type(self.mask_net)}")

    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­ï¼Œé›†æˆå¯¹æ¯”å­¦ä¹ åŠŸèƒ½ï¼Œæ”¯æŒMT
        é¿å…é‡å¤è®¡ç®—ï¼Œç›´æ¥ä»ä¸­é—´ç»“æœè·å–logits
        
        Args:
            inputs: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„å­—å…¸
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œä¸­é—´ç»“æœçš„å­—å…¸
        """
        X = self.get_inputs(inputs)
        
        if not self.use_domain_aware_structure:
            # éMTæ¨¡å¼ï¼šä¸€æ¬¡æ€§è®¡ç®—logitså’Œy_predï¼Œé¿å…é‡å¤è®¡ç®—
            logits, y_pred = self._forward_with_logits(X)
            return_dict = {"y_pred": y_pred, "logits": logits}
        else:
            # MTæ¨¡å¼ï¼šä¸€æ¬¡æ€§è®¡ç®—logitså’Œy_predï¼Œé¿å…è°ƒç”¨çˆ¶ç±»forwardå¯¼è‡´çš„é‡å¤è®¡ç®—
            logits, y_pred = self._forward_with_logits_mt(X)
            return_dict = {"y_pred": y_pred, "logits": logits}
        
        # è·å–ç»„ä¿¡æ¯ï¼ˆåŸºäºis_personalizationç‰¹å¾ï¼‰
        if self.training and 'is_personalization' in inputs:
            group_ids = self.get_group_ids(inputs)
            if group_ids is not None:
                return_dict["group_ids"] = group_ids
        
        # å¦‚æœå¯ç”¨CLï¼Œä»å·²è®¡ç®—çš„embeddingä¸­æå–ç‰¹å¾åµŒå…¥ï¼Œé¿å…é‡å¤è®¡ç®—
        if self.training and (self.feature_alignment_loss_weight > 0 or
                             self.field_uniformity_loss_weight > 0):
            # å¤ç”¨å·²ç»è®¡ç®—çš„embeddingï¼Œé¿å…é‡æ–°è®¡ç®—
            feature_emb = self.embedding_layer(X)
            feature_embeddings = self._extract_feature_embeddings_from_tensor(feature_emb, X)
            return_dict["feature_embeddings"] = feature_embeddings
        
        # å¦‚æœå¯ç”¨ä¸ªæ€§åŒ–æ©ç ï¼Œç”Ÿæˆå¯¹æ¯”è§†å›¾ï¼Œå¤ç”¨å·²è®¡ç®—çš„logits
        if self.training and self.use_cl_mask:
            # h1ä½¿ç”¨å·²è®¡ç®—çš„logitsï¼Œé¿å…é‡å¤è®¡ç®—
            h1_logits = logits
            
            # h2åªéœ€è¦è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
            h2_logits = self._compute_non_personalized_logits(X)
            
            if h1_logits is not None and h2_logits is not None:
                return_dict["h1_logits"] = h1_logits
                return_dict["h2_logits"] = h2_logits
        
        return return_dict
    
    def _extract_feature_embeddings_from_tensor(self, feature_emb, X):
        """
        ä»å·²è®¡ç®—çš„ç‰¹å¾åµŒå…¥å¼ é‡ä¸­æå–å„ä¸ªç‰¹å¾çš„åµŒå…¥ï¼Œé¿å…é‡å¤è®¡ç®—
        
        Args:
            feature_emb: å·²è®¡ç®—çš„ç‰¹å¾åµŒå…¥å¼ é‡ [batch_size, num_fields, embedding_dim]
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            dict: {feature_name: embedding_tensor}
        """
        feature_embeddings = {}
        feature_names = list(X.keys())
        
        # å°†ç‰¹å¾åµŒå…¥æŒ‰å­—æ®µåˆ†å‰²
        if feature_emb.dim() == 3:  # [batch_size, num_fields, embedding_dim]
            feat_list = feature_emb.chunk(len(feature_names), dim=1)
            for i, feature_name in enumerate(feature_names):
                if i < len(feat_list):
                    feature_embeddings[feature_name] = feat_list[i].squeeze(1)
        else:
            # å¦‚æœæ˜¯å±•å¹³çš„ï¼Œéœ€è¦é‡æ–°è®¡ç®—å„ä¸ªç‰¹å¾çš„åµŒå…¥
            feature_embeddings = self.get_feature_embeddings(self.embedding_layer, X)
            
        return feature_embeddings
    
    def _compute_non_personalized_logits(self, X):
        """
        è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
        
        Args:
            X: è¾“å…¥ç‰¹å¾å­—å…¸
            
        Returns:
            torch.Tensor: éä¸ªæ€§åŒ–è§†å›¾çš„logits
        """
        if not self.use_cl_mask or self.mask_type != 'Personalisation':
            return None
        
        # ç”Ÿæˆéä¸ªæ€§åŒ–è§†å›¾
        non_personalized_X = {}
        for feature_name, feature_value in X.items():
            if feature_name in self.personalization_feature_list:
                # ä½¿ç”¨ç½®é›¶ç­–ç•¥ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•å…¶ä»–ç­–ç•¥ï¼‰
                non_personalized_X[feature_name] = torch.zeros_like(feature_value)
            else:
                non_personalized_X[feature_name] = feature_value
        
        # è®¡ç®—éä¸ªæ€§åŒ–è§†å›¾çš„logits
        if not self.use_domain_aware_structure:
            # éMTæ¨¡å¼
            h2_logits, _ = self._forward_with_logits(non_personalized_X)
        else:
            # MTæ¨¡å¼
            h2_logits, _ = self._forward_with_logits_mt(non_personalized_X)
        
        return h2_logits
    
    def add_loss(self, return_dict, y_true):
        """
        è®¡ç®—åŒ…å«å¯¹æ¯”å­¦ä¹ çš„æ€»æŸå¤±
        
        Args:
            return_dict: forwardæ–¹æ³•çš„è¿”å›å­—å…¸
            y_true: çœŸå®æ ‡ç­¾
            
        Returns:
            torch.Tensor: æ€»æŸå¤±
        """
        # åŸºç¡€MaskNetæŸå¤±
        base_loss = self.loss_fn(return_dict["y_pred"], y_true, reduction='mean')
        
        # å¦‚æœä¸åœ¨è®­ç»ƒæ¨¡å¼æˆ–æ²¡æœ‰å¯ç”¨CLï¼Œç›´æ¥è¿”å›åŸºç¡€æŸå¤±
        if not self.training or not self.use_cl_loss:
            return base_loss
        
        # æå–CLç›¸å…³çš„ä¸­é—´ç»“æœ
        feature_embeddings = return_dict.get("feature_embeddings", None)
        h1_logits = return_dict.get("h1_logits", None)
        h2_logits = return_dict.get("h2_logits", None)
        group_ids = return_dict.get("group_ids", None)
        
        # è®¡ç®—å®Œæ•´çš„CLæŸå¤±
        total_loss = self.compute_cl_loss(
            base_loss=base_loss,
            feature_embeddings=feature_embeddings,
            h1_logits=h1_logits,
            h2_logits=h2_logits,
            labels=y_true,
            group_ids=group_ids
        )
        
        return total_loss 